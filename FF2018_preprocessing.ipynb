{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##<script>\n",
    "##  jQuery(document).ready(function($) {  \n",
    "##  \n",
    "##  $(window).on('load', function(){\n",
    "##    $('#preloader').fadeOut('slow',function(){$(this).remove();});\n",
    "##  });\n",
    "##  \n",
    "##  });\n",
    "##</script>\n",
    "##\n",
    "##<style type=\"text/css\">\n",
    "##  div#preloader { position: fixed; \n",
    "##      left: 0; \n",
    "##      top: 0; \n",
    "##      z-index: 999; \n",
    "##      width: 100%; \n",
    "##      height: 100%; \n",
    "##      overflow: visible; \n",
    "##      background: #fff url('http://preloaders.net/preloaders/720/Moving%20line.gif') no-repeat center center; \n",
    "##  }\n",
    "##\n",
    "##</style>\n",
    "##\n",
    "##<div id=\"preloader\">\n",
    "##\n",
    "##</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food Framing Preprocessing Script "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "## [1. Load Data](#LoadData)\n",
    "## [2. Eye Blink Check](#CheckBlinks)\n",
    "## [3. Calculate GSF](#GSF)\n",
    "## [4. Calculate DDT](#DDT)\n",
    "## [5. Summary Measures](#summary)\n",
    "## [6. Compute z and familiarity variables](#zfam)\n",
    "## [7. Median Split](#split) \n",
    "## [8. Participant Exclusion](#PartEx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LoadData'></a>\n",
    "# 1. Loading Pilot Data\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load eyetracker data reports generated by EyeLink DataViewer.\n",
    "#### Loading Interest Area (IA) and Saccade (Sacc) reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/Users/pradyumna/OneDrive - University College London/PhD Projects/FoodFramingValue/FF2018_ParticipantData/Output/IA_report_first1500ms.txt' does not exist: b'/Users/pradyumna/OneDrive - University College London/PhD Projects/FoodFramingValue/FF2018_ParticipantData/Output/IA_report_first1500ms.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-50c3128489ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpathSacc\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"/Users/pradyumna/OneDrive - University College London/PhD Projects/FoodFramingValue/FF2018_ParticipantData/Output/fixation_report_first1500ms.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# load csv for IA and Sacc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpX_IAdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathIA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpX_SACCdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathSacc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Concatenate to generate frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/Users/pradyumna/OneDrive - University College London/PhD Projects/FoodFramingValue/FF2018_ParticipantData/Output/IA_report_first1500ms.txt' does not exist: b'/Users/pradyumna/OneDrive - University College London/PhD Projects/FoodFramingValue/FF2018_ParticipantData/Output/IA_report_first1500ms.txt'"
     ]
    }
   ],
   "source": [
    "#Select path for IA and Sacc\n",
    "pathIA =\"/Users/pradyumna/OneDrive - University College London/PhD Projects/FoodFramingValue/FF2018_ParticipantData/Output/IA_report_first1500ms.txt\"\n",
    "pathSacc =\"/Users/pradyumna/OneDrive - University College London/PhD Projects/FoodFramingValue/FF2018_ParticipantData/Output/fixation_report_first1500ms.txt\"\n",
    "# load csv for IA and Sacc\n",
    "pX_IAdata = [pd.read_csv(pathIA, sep=\"\\t\")]\n",
    "pX_SACCdata = [pd.read_csv(pathSacc, sep=\"\\t\")]\n",
    "# Concatenate to generate frames\n",
    "IAdata = pd.concat(pX_IAdata)\n",
    "SACCdata = pd.concat(pX_SACCdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe with interest Area information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IAdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe with saccade information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SACCdata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1481aa883292>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSACCdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'SACCdata' is not defined"
     ]
    }
   ],
   "source": [
    "SACCdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of participants and trials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p1' 'p5' 'p6' 'p7' 'p8' 'p9' 'p10' 'p11' 'p12' 'p13' 'p14' 'p15' 'p16'\n",
      " 'p17' 'p18' 'p19' 'p20' 'p21' 'p22' 'p23' 'p24' 'p25' 'p26' 'p27' 'p28'\n",
      " 'p29' 'p30' 'p31' 'p32' 'p33' 'p34' 'p35' 'p36' 'p37' 'p38' 'p39' 'p40'\n",
      " 'p41' 'p42' 'p43']\n",
      "Number of participants: 40\n",
      "Number of trials: 240\n"
     ]
    }
   ],
   "source": [
    "participant = IAdata['DATA_FILE'].unique()\n",
    "participant_names = IAdata['DATA_FILE'].unique()\n",
    "\n",
    "#Extract just names of participant (for folder name in future steps)\n",
    "yy=0\n",
    "for xx in participant_names:\n",
    "    participant_names[yy]=xx.rstrip(\".edf\")\n",
    "    yy+=1\n",
    "print (participant_names)\n",
    "\n",
    "trial_n = IAdata['trial'].unique()\n",
    "\n",
    "print(\"Number of participants:\", len(participant))\n",
    "print (\"Number of trials:\", len(trial_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='CheckBlinks'></a>\n",
    "# 2. Check Blinks across participants\n",
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading dataframe containing trial information for all the participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PathTrialReport =\"/Users/pradyumna/OneDrive - University College London/PhD Projects/FoodFramingValue/Reports/TrialReport_Blinks_v1.txt\"\n",
    "TrialRepFrame = [pd.read_csv(PathTrialReport, sep=\"\\t\")]\n",
    "TrialDataFrame = pd.concat(TrialRepFrame)\n",
    "#TrialDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting information for each participant\n",
    "BlinkMatrix = []\n",
    "partBlink = TrialDataFrame['DATA_FILE'].unique()\n",
    "for k in range(0,len(partBlink)): \n",
    "    pX_Blinks = TrialDataFrame.loc[(TrialDataFrame['DATA_FILE']==partBlink[k]) & (TrialDataFrame['trial_type']== 'Experiment')]\n",
    "\n",
    "    Blink_Count_trial =  pX_Blinks['BLINK_COUNT']\n",
    "    Blink_Average_trial =  pd.to_numeric(pX_Blinks['AVERAGE_BLINK_DURATION'], errors='coerce')\n",
    "    Duration_trial =  pd.to_numeric(pX_Blinks['DURATION'], errors='coerce')\n",
    "\n",
    "    Blink_Count_part = Blink_Count_trial.mean()\n",
    "    Blink_CountSD_part = Blink_Count_trial.std()\n",
    "    Blink_Average_part = Blink_Average_trial.mean(skipna = True)\n",
    "    Blink_AverageSD_part = Blink_Average_trial.std(skipna = True)\n",
    "    Duration_part = Duration_trial.mean(skipna = True)\n",
    "\n",
    "    BlinkMatrix.append([partBlink[k] , Blink_Count_part, Blink_CountSD_part, Blink_Average_part, Blink_AverageSD_part, Duration_part]) \n",
    "\n",
    "BlinkMatrix=pd.DataFrame(BlinkMatrix,columns=['Participant', 'BlinkCountAverage', 'BlinkCountStd', 'BlinkTimeAverage', 'BlinkTimeAverageStd','TrialDurationAverage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BlinkCountAverage : mean blink number during all the trials for each participant\n",
    "##### BlinkTimeAverage : mean duration (ms) of a blink event for each participant\n",
    "##### TrialDurationAverage : mean duration (ms) of the trial for each participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant</th>\n",
       "      <th>BlinkCountAverage</th>\n",
       "      <th>BlinkCountStd</th>\n",
       "      <th>BlinkTimeAverage</th>\n",
       "      <th>BlinkTimeAverageStd</th>\n",
       "      <th>TrialDurationAverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p1.edf</td>\n",
       "      <td>0.370833</td>\n",
       "      <td>0.672209</td>\n",
       "      <td>92.042254</td>\n",
       "      <td>80.064957</td>\n",
       "      <td>7997.291667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p5.edf</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>1.163537</td>\n",
       "      <td>189.220090</td>\n",
       "      <td>88.097275</td>\n",
       "      <td>6377.383333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p6.edf</td>\n",
       "      <td>1.070833</td>\n",
       "      <td>0.881696</td>\n",
       "      <td>115.250879</td>\n",
       "      <td>38.817476</td>\n",
       "      <td>6987.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p7.edf</td>\n",
       "      <td>1.195833</td>\n",
       "      <td>1.038578</td>\n",
       "      <td>135.605978</td>\n",
       "      <td>78.495276</td>\n",
       "      <td>7628.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p8.edf</td>\n",
       "      <td>1.054167</td>\n",
       "      <td>1.128364</td>\n",
       "      <td>106.463613</td>\n",
       "      <td>17.419231</td>\n",
       "      <td>6757.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>p9.edf</td>\n",
       "      <td>3.137500</td>\n",
       "      <td>1.214388</td>\n",
       "      <td>132.536250</td>\n",
       "      <td>46.917266</td>\n",
       "      <td>6479.941667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>p10.edf</td>\n",
       "      <td>0.445833</td>\n",
       "      <td>0.624787</td>\n",
       "      <td>133.271739</td>\n",
       "      <td>71.207189</td>\n",
       "      <td>6942.641667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>p11.edf</td>\n",
       "      <td>0.154167</td>\n",
       "      <td>0.538461</td>\n",
       "      <td>169.118929</td>\n",
       "      <td>317.848335</td>\n",
       "      <td>6862.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>p12.edf</td>\n",
       "      <td>0.329167</td>\n",
       "      <td>0.610219</td>\n",
       "      <td>208.746032</td>\n",
       "      <td>123.446602</td>\n",
       "      <td>6227.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>p13.edf</td>\n",
       "      <td>2.287500</td>\n",
       "      <td>1.041327</td>\n",
       "      <td>157.710462</td>\n",
       "      <td>48.511869</td>\n",
       "      <td>7254.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>p14.edf</td>\n",
       "      <td>1.579167</td>\n",
       "      <td>1.019672</td>\n",
       "      <td>167.421549</td>\n",
       "      <td>72.099945</td>\n",
       "      <td>7105.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>p15.edf</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.559188</td>\n",
       "      <td>221.297937</td>\n",
       "      <td>54.817829</td>\n",
       "      <td>7174.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>p16.edf</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.858951</td>\n",
       "      <td>193.287234</td>\n",
       "      <td>183.025021</td>\n",
       "      <td>7530.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>p17.edf</td>\n",
       "      <td>0.408333</td>\n",
       "      <td>0.606738</td>\n",
       "      <td>144.575581</td>\n",
       "      <td>66.939108</td>\n",
       "      <td>6937.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>p18.edf</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>310.895244</td>\n",
       "      <td>469.310471</td>\n",
       "      <td>8053.216667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>p19.edf</td>\n",
       "      <td>0.654167</td>\n",
       "      <td>0.829114</td>\n",
       "      <td>92.862773</td>\n",
       "      <td>45.411701</td>\n",
       "      <td>7465.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>p20.edf</td>\n",
       "      <td>0.904167</td>\n",
       "      <td>1.068366</td>\n",
       "      <td>168.825733</td>\n",
       "      <td>106.994711</td>\n",
       "      <td>6855.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>p21.edf</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.527338</td>\n",
       "      <td>122.266140</td>\n",
       "      <td>65.527157</td>\n",
       "      <td>8362.858333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>p22.edf</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.640291</td>\n",
       "      <td>391.799557</td>\n",
       "      <td>247.244498</td>\n",
       "      <td>7200.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>p23.edf</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.179881</td>\n",
       "      <td>234.500000</td>\n",
       "      <td>46.840154</td>\n",
       "      <td>6707.991667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>p24.edf</td>\n",
       "      <td>1.504167</td>\n",
       "      <td>1.237483</td>\n",
       "      <td>229.627766</td>\n",
       "      <td>126.828030</td>\n",
       "      <td>7080.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>p25.edf</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.849717</td>\n",
       "      <td>171.074257</td>\n",
       "      <td>150.713171</td>\n",
       "      <td>6580.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>p26.edf</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.663399</td>\n",
       "      <td>111.820490</td>\n",
       "      <td>32.169614</td>\n",
       "      <td>6387.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>p27.edf</td>\n",
       "      <td>1.683333</td>\n",
       "      <td>0.919190</td>\n",
       "      <td>236.722991</td>\n",
       "      <td>59.407527</td>\n",
       "      <td>8059.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>p28.edf</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.584254</td>\n",
       "      <td>409.310278</td>\n",
       "      <td>267.675617</td>\n",
       "      <td>6847.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>p29.edf</td>\n",
       "      <td>3.050000</td>\n",
       "      <td>1.392448</td>\n",
       "      <td>388.157531</td>\n",
       "      <td>194.023461</td>\n",
       "      <td>10753.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>p30.edf</td>\n",
       "      <td>1.408333</td>\n",
       "      <td>0.858098</td>\n",
       "      <td>282.200267</td>\n",
       "      <td>168.626255</td>\n",
       "      <td>5842.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>p31.edf</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.630675</td>\n",
       "      <td>231.064432</td>\n",
       "      <td>289.957400</td>\n",
       "      <td>8477.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>p32.edf</td>\n",
       "      <td>1.708333</td>\n",
       "      <td>1.471605</td>\n",
       "      <td>160.882523</td>\n",
       "      <td>145.391669</td>\n",
       "      <td>8605.141667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>p33.edf</td>\n",
       "      <td>1.387500</td>\n",
       "      <td>1.232344</td>\n",
       "      <td>189.151574</td>\n",
       "      <td>157.635533</td>\n",
       "      <td>8065.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>p34.edf</td>\n",
       "      <td>1.670833</td>\n",
       "      <td>3.491170</td>\n",
       "      <td>106.217762</td>\n",
       "      <td>116.415537</td>\n",
       "      <td>7976.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>p35.edf</td>\n",
       "      <td>1.962500</td>\n",
       "      <td>1.986000</td>\n",
       "      <td>316.160181</td>\n",
       "      <td>174.057422</td>\n",
       "      <td>8300.383333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>p36.edf</td>\n",
       "      <td>1.795833</td>\n",
       "      <td>0.995798</td>\n",
       "      <td>437.837040</td>\n",
       "      <td>195.776036</td>\n",
       "      <td>9043.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>p37.edf</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.714648</td>\n",
       "      <td>144.996354</td>\n",
       "      <td>58.323358</td>\n",
       "      <td>6054.741667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>p38.edf</td>\n",
       "      <td>1.837500</td>\n",
       "      <td>1.032247</td>\n",
       "      <td>144.564915</td>\n",
       "      <td>46.191611</td>\n",
       "      <td>9101.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>p39.edf</td>\n",
       "      <td>2.662500</td>\n",
       "      <td>1.416671</td>\n",
       "      <td>119.632468</td>\n",
       "      <td>50.133899</td>\n",
       "      <td>7554.341667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>p40.edf</td>\n",
       "      <td>3.641667</td>\n",
       "      <td>1.965373</td>\n",
       "      <td>450.404076</td>\n",
       "      <td>323.733230</td>\n",
       "      <td>9582.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>p41.edf</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>1.261039</td>\n",
       "      <td>99.079431</td>\n",
       "      <td>62.697864</td>\n",
       "      <td>6218.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>p42.edf</td>\n",
       "      <td>0.670833</td>\n",
       "      <td>0.693653</td>\n",
       "      <td>147.500000</td>\n",
       "      <td>96.762672</td>\n",
       "      <td>7381.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>p43.edf</td>\n",
       "      <td>1.195833</td>\n",
       "      <td>1.371857</td>\n",
       "      <td>241.837563</td>\n",
       "      <td>203.208511</td>\n",
       "      <td>8897.641667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Participant  BlinkCountAverage  BlinkCountStd  BlinkTimeAverage  \\\n",
       "0       p1.edf           0.370833       0.672209         92.042254   \n",
       "1       p5.edf           1.687500       1.163537        189.220090   \n",
       "2       p6.edf           1.070833       0.881696        115.250879   \n",
       "3       p7.edf           1.195833       1.038578        135.605978   \n",
       "4       p8.edf           1.054167       1.128364        106.463613   \n",
       "5       p9.edf           3.137500       1.214388        132.536250   \n",
       "6      p10.edf           0.445833       0.624787        133.271739   \n",
       "7      p11.edf           0.154167       0.538461        169.118929   \n",
       "8      p12.edf           0.329167       0.610219        208.746032   \n",
       "9      p13.edf           2.287500       1.041327        157.710462   \n",
       "10     p14.edf           1.579167       1.019672        167.421549   \n",
       "11     p15.edf           0.716667       0.559188        221.297937   \n",
       "12     p16.edf           0.583333       0.858951        193.287234   \n",
       "13     p17.edf           0.408333       0.606738        144.575581   \n",
       "14     p18.edf           0.520833       0.980769        310.895244   \n",
       "15     p19.edf           0.654167       0.829114         92.862773   \n",
       "16     p20.edf           0.904167       1.068366        168.825733   \n",
       "17     p21.edf           0.262500       0.527338        122.266140   \n",
       "18     p22.edf           0.758333       0.640291        391.799557   \n",
       "19     p23.edf           0.033333       0.179881        234.500000   \n",
       "20     p24.edf           1.504167       1.237483        229.627766   \n",
       "21     p25.edf           0.812500       0.849717        171.074257   \n",
       "22     p26.edf           0.691667       0.663399        111.820490   \n",
       "23     p27.edf           1.683333       0.919190        236.722991   \n",
       "24     p28.edf           0.208333       0.584254        409.310278   \n",
       "25     p29.edf           3.050000       1.392448        388.157531   \n",
       "26     p30.edf           1.408333       0.858098        282.200267   \n",
       "27     p31.edf           0.437500       0.630675        231.064432   \n",
       "28     p32.edf           1.708333       1.471605        160.882523   \n",
       "29     p33.edf           1.387500       1.232344        189.151574   \n",
       "30     p34.edf           1.670833       3.491170        106.217762   \n",
       "31     p35.edf           1.962500       1.986000        316.160181   \n",
       "32     p36.edf           1.795833       0.995798        437.837040   \n",
       "33     p37.edf           0.937500       0.714648        144.996354   \n",
       "34     p38.edf           1.837500       1.032247        144.564915   \n",
       "35     p39.edf           2.662500       1.416671        119.632468   \n",
       "36     p40.edf           3.641667       1.965373        450.404076   \n",
       "37     p41.edf           0.937500       1.261039         99.079431   \n",
       "38     p42.edf           0.670833       0.693653        147.500000   \n",
       "39     p43.edf           1.195833       1.371857        241.837563   \n",
       "\n",
       "    BlinkTimeAverageStd  TrialDurationAverage  \n",
       "0             80.064957           7997.291667  \n",
       "1             88.097275           6377.383333  \n",
       "2             38.817476           6987.525000  \n",
       "3             78.495276           7628.466667  \n",
       "4             17.419231           6757.916667  \n",
       "5             46.917266           6479.941667  \n",
       "6             71.207189           6942.641667  \n",
       "7            317.848335           6862.675000  \n",
       "8            123.446602           6227.416667  \n",
       "9             48.511869           7254.366667  \n",
       "10            72.099945           7105.350000  \n",
       "11            54.817829           7174.583333  \n",
       "12           183.025021           7530.125000  \n",
       "13            66.939108           6937.725000  \n",
       "14           469.310471           8053.216667  \n",
       "15            45.411701           7465.366667  \n",
       "16           106.994711           6855.375000  \n",
       "17            65.527157           8362.858333  \n",
       "18           247.244498           7200.583333  \n",
       "19            46.840154           6707.991667  \n",
       "20           126.828030           7080.550000  \n",
       "21           150.713171           6580.466667  \n",
       "22            32.169614           6387.275000  \n",
       "23            59.407527           8059.958333  \n",
       "24           267.675617           6847.791667  \n",
       "25           194.023461          10753.625000  \n",
       "26           168.626255           5842.625000  \n",
       "27           289.957400           8477.450000  \n",
       "28           145.391669           8605.141667  \n",
       "29           157.635533           8065.966667  \n",
       "30           116.415537           7976.850000  \n",
       "31           174.057422           8300.383333  \n",
       "32           195.776036           9043.208333  \n",
       "33            58.323358           6054.741667  \n",
       "34            46.191611           9101.775000  \n",
       "35            50.133899           7554.341667  \n",
       "36           323.733230           9582.416667  \n",
       "37            62.697864           6218.933333  \n",
       "38            96.762672           7381.041667  \n",
       "39           203.208511           8897.641667  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BlinkMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (*) Participant 40 (p40.edf) is the one with the highest number of blink events and the one with longer average blinking among all the participants. Additionally, he lost one trial beacuse of no fixations. p40.edf is taken out from posterior analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary subject extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminated participant  36\n",
      "Eliminated participant name p40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['p1.edf', 'p5.edf', 'p6.edf', 'p7.edf', 'p8.edf', 'p9.edf',\n",
       "       'p10.edf', 'p11.edf', 'p12.edf', 'p13.edf', 'p14.edf', 'p15.edf',\n",
       "       'p16.edf', 'p17.edf', 'p18.edf', 'p19.edf', 'p20.edf', 'p21.edf',\n",
       "       'p22.edf', 'p23.edf', 'p24.edf', 'p25.edf', 'p26.edf', 'p27.edf',\n",
       "       'p28.edf', 'p29.edf', 'p30.edf', 'p31.edf', 'p32.edf', 'p33.edf',\n",
       "       'p34.edf', 'p35.edf', 'p36.edf', 'p37.edf', 'p38.edf', 'p39.edf',\n",
       "       'p41.edf', 'p42.edf', 'p43.edf'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Subject 40 becuase of sleepineess during the experiment\n",
    "delete_index = []\n",
    "for i in range(len(participant)):\n",
    "    if participant_names[i]=='p40':\n",
    "        delete_index = i\n",
    "print('Eliminated participant ', delete_index)\n",
    "print('Eliminated participant name'  ,participant_names[delete_index])\n",
    "        \n",
    "participant_names = np.delete(participant_names, [delete_index], None)\n",
    "participant = np.delete(participant, [delete_index], None)\n",
    "participant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='GSF'></a>\n",
    "# 3. Calculate GSF for each participant and trial\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'CURRENT_SAC_START_INTEREST_AREA_ID'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'CURRENT_SAC_START_INTEREST_AREA_ID'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-998b36373a9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# considered direct glance transition between IA 1 (left) and IA 2 (right)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mbooatstart_p1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGSF_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CURRENT_SAC_START_INTEREST_AREA_ID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mbooatend_p1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGSF_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CURRENT_SAC_END_INTEREST_AREA_ID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mbooatstart_p2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGSF_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CURRENT_SAC_START_INTEREST_AREA_ID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'CURRENT_SAC_START_INTEREST_AREA_ID'"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "GSF_matrix = []\n",
    "\n",
    "#Run for each individual participant\n",
    "for i in range(len(participant)):\n",
    "    #Extract the number of trials for this participant\n",
    "    #pX_indSubj = SACCdata.loc[(SACCdata['DATA_FILE']==participant[i]) & (SACCdata['trial_type']== 'Experiment')]\n",
    "    #Add number of trials for the experiment\n",
    "    #trial_n = pX_indSubj['trial'].unique()\n",
    "    trial_n = range(240)\n",
    "    for  j in range(len(trial_n)):\n",
    "            # extract saccade data for this participant\n",
    "            GSF_data=SACCdata.loc[(SACCdata['DATA_FILE']==participant[i]) & (SACCdata['trial']== j+1) & (SACCdata['trial_type']== 'Experiment')]\n",
    "\n",
    "\n",
    "            # considered direct glance transition between IA 1 (left) and IA 2 (right)\n",
    "            booatstart_p1=GSF_data['CURRENT_SAC_START_INTEREST_AREA_ID'] == '2'\n",
    "            booatend_p1=GSF_data['CURRENT_SAC_END_INTEREST_AREA_ID'] == '1'\n",
    "            booatstart_p2=GSF_data['CURRENT_SAC_START_INTEREST_AREA_ID'] == '1'\n",
    "            booatend_p2=GSF_data['CURRENT_SAC_END_INTEREST_AREA_ID'] == '2'            \n",
    "            \n",
    "            #Count shift only if the saccade goes from one area to the other (right to left or left to right item)\n",
    "            shifts_p1= booatstart_p1 & booatend_p1\n",
    "            shifts_p2= booatstart_p2 & booatend_p2\n",
    "\n",
    "            shifts_total = shifts_p1 | shifts_p2\n",
    "            #Count the total number of shifts in this trial\n",
    "            trialGSF = sum(shifts_total*1)\n",
    "            \n",
    "            #Extract the region (left or right) for the last fixation\n",
    "            go_back = 1\n",
    "            tail_fix = GSF_data['CURRENT_SAC_END_INTEREST_AREA_ID'].tail(go_back)  \n",
    "            \n",
    "            # If we have not fixation information we don't skip it, we fill with zeros\n",
    "            if GSF_data.empty:\n",
    "                print ('skipped: participant:',participant[i],'trial',j+1)\n",
    "                #continue\n",
    "            \n",
    "            ## If the sequence is empty, we assign 0 to tail\n",
    "            if tail_fix.empty:\n",
    "                tail_fix = 0\n",
    "            else:\n",
    "                tail_fix = tail_fix.values[0]\n",
    "            \n",
    "            while ('.'== tail_fix) | ('3'== tail_fix):\n",
    "                go_back = go_back + 1\n",
    "                tail_fix = GSF_data['CURRENT_SAC_END_INTEREST_AREA_ID'].tail(go_back)\n",
    "                #Since tail gets the last n subjects we need to just pick the one on top\n",
    "                tail_fix_aux = tail_fix.values\n",
    "                tail_fix = tail_fix_aux[0]\n",
    "            \n",
    "            #add GSF trial info to a new row if the matrix\n",
    "            GSF_matrix.append([participant[i], j+1, trialGSF, tail_fix]) #trial number, trial GSF, last_fixation\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize GSF_matrix\n",
    "#GSF_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='DDT'></a>\n",
    "# 4. Calculate DDT for each participant and trial\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract measures of dwell time (DT) per Interest Area (IA) in a particular trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information contained in IA report \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DATA_FILE',\n",
       " 'IA_ID',\n",
       " 'trial',\n",
       " 'Triplet_Identity',\n",
       " 'trial_type',\n",
       " 'LL_Item',\n",
       " 'LL_Value',\n",
       " 'LR_Item',\n",
       " 'LR_Value',\n",
       " 'Chosen_Img',\n",
       " 'CHOICE_KEYPRESS',\n",
       " 'CHOICE_RT',\n",
       " 'CONF',\n",
       " 'CONF_RT',\n",
       " 'Block_Cond',\n",
       " 'IA_DWELL_TIME',\n",
       " 'IA_DWELL_TIME_%',\n",
       " 'IA_FIXATION_COUNT',\n",
       " 'IA_FIXATION_%',\n",
       " 'IA_MAX_FIX_PUPIL_SIZE',\n",
       " 'IA_AVERAGE_FIX_PUPIL_SIZE',\n",
       " 'IA_MIN_FIX_PUPIL_SIZE',\n",
       " 'IA_RUN_COUNT',\n",
       " 'TRIAL_DWELL_TIME',\n",
       " 'TRIAL_FIXATION_COUNT']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IA1: left item ; IA2 : right item ; IA3: condition icon\n",
    "IA1_data=IAdata.loc[(IAdata['DATA_FILE']==participant[i]) & (IAdata['trial']== j+1) & (IAdata['IA_ID']== 1) & (IAdata['trial_type']== 'Experiment')]\n",
    "IA2_data=IAdata.loc[(IAdata['DATA_FILE']==participant[i]) & (IAdata['trial']== j+1) & (IAdata['IA_ID']== 2) & (IAdata['trial_type']== 'Experiment')]\n",
    "IA3_data=IAdata.loc[(IAdata['DATA_FILE']==participant[i]) & (IAdata['trial']== j+1) & (IAdata['IA_ID']== 3) & (IAdata['trial_type']== 'Experiment')]\n",
    "\n",
    "print (\"Information contained in IA report \")\n",
    "list(IA1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "area1 0 area2 0\n",
      "skipped: participant: 1 trial 167\n",
      "True\n",
      "area1 0 area2 0\n",
      "skipped: participant: 1 trial 168\n",
      "True\n",
      "area1 0 area2 0\n",
      "skipped: participant: 1 trial 169\n",
      "True\n",
      "area1 0 area2 0\n",
      "skipped: participant: 1 trial 208\n",
      "True\n",
      "area1 0 area2 0\n",
      "skipped: participant: 1 trial 212\n",
      "True\n",
      "area1 0 area2 0\n",
      "skipped: participant: 12 trial 10\n",
      "True\n",
      "area1 0 area2 0\n",
      "skipped: participant: 12 trial 32\n",
      "True\n",
      "area1 0 area2 0\n",
      "skipped: participant: 14 trial 121\n",
      "True\n",
      "area1 0 area2 0\n",
      "skipped: participant: 14 trial 220\n",
      "True\n",
      "area1 0 area2 0\n",
      "skipped: participant: 18 trial 120\n",
      "True\n",
      "area1 0 area2 0\n",
      "skipped: participant: 18 trial 228\n",
      "True\n",
      "area1 0 area2 0\n",
      "skipped: participant: 26 trial 51\n",
      "True\n",
      "area1 0 area2 0\n",
      "skipped: participant: 29 trial 135\n",
      "True\n",
      "area1 0 area2 0\n",
      "skipped: participant: 29 trial 201\n",
      "True\n",
      "area1 0 area2 0\n",
      "skipped: participant: 29 trial 206\n",
      "True\n",
      "area1 0 area2 0\n",
      "skipped: participant: 38 trial 10\n",
      "True\n",
      "area1 0 area2 0\n",
      "skipped: participant: 38 trial 94\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "DDT_matrix = []\n",
    "\n",
    "#Extract information for each participant. Behavioural parameters also extracted from this report.\n",
    "for i in range(len(participant)):\n",
    "    #pX_indSubj = IAdata.loc[(IAdata['DATA_FILE']==participant[i]) & (pX_IAdata['trial_type']== 'Experiment')]\n",
    "    #Add number of trials for the experiment\n",
    "    #trial_n = pX_indSubj['trial'].unique()\n",
    "    trial_n = range(240)\n",
    "    for  j in range(len(trial_n)):\n",
    "            #Extract DT information for the trial IA1 :left ; IA2 : right\n",
    "            IA1_data=IAdata.loc[(IAdata['DATA_FILE']==participant[i]) & (IAdata['trial']== j+1) & (IAdata['IA_ID']== 1) & (IAdata['trial_type']== 'Experiment')]\n",
    "            IA2_data=IAdata.loc[(IAdata['DATA_FILE']==participant[i]) & (IAdata['trial']== j+1) & (IAdata['IA_ID']== 2) & (IAdata['trial_type']== 'Experiment')]\n",
    "\n",
    "            \n",
    "            #Save additional behavioural parameters\n",
    "            trial_PairID = IA1_data['Triplet_Identity'].values[0]\n",
    "            trial_LL_Item = IA1_data['LL_Item'].values[0]\n",
    "            trial_LL_Value = IA1_data['LL_Value'].values[0]\n",
    "            trial_LR_Item = IA1_data['LR_Item'].values[0]\n",
    "            trial_LR_Value = IA1_data['LR_Value'].values[0]\n",
    "            if IA1_data['CHOICE_KEYPRESS'].values[0] == 'm':\n",
    "                trial_ChosenItm = 1  # 1: selected item is Right ; 0: selected item is Left\n",
    "            else: \n",
    "                trial_ChosenItm = 0\n",
    "            trial_ChoiceRT = IA1_data['CHOICE_RT'].values[0]\n",
    "            trial_Conf = IA1_data['CONF'].values[0]\n",
    "            trial_ConfRT = IA1_data['CONF_RT'].values[0]\n",
    "            trial_BlockCond = IA1_data['Block_Cond'].values[0] #1:like 2: dislike\n",
    "            \n",
    "            #Calculate DDT values for the trial\n",
    "            IA_1_DT = IA1_data['IA_DWELL_TIME'].values[0]\n",
    "            IA_2_DT = IA2_data['IA_DWELL_TIME'].values[0]\n",
    "            \n",
    "            # If participant did not spend time in the Interest Areas we should skip it\n",
    "            if (IA_1_DT == 0) & (IA_2_DT == 0):\n",
    "                print(IA_1_DT == 0 & IA_2_DT == 0)\n",
    "                print ('area1',IA_1_DT,'area2',IA_2_DT)\n",
    "                print ('skipped: participant:',i,'trial',j+1)\n",
    "                continue\n",
    "            \n",
    "            trialDDT = -IA_1_DT + IA_2_DT # since DDT_GSF_Zvalue_trial_part_v2.csv we use RIGHT - LEFT\n",
    "            \n",
    "            #define matrix DDT + behavioural parameters\n",
    "            DDT_matrix.append([i+1, j+1, trial_PairID, trial_LL_Item, trial_LL_Value, trial_LR_Item, trial_LR_Value, trial_ChosenItm, trial_ChoiceRT, trial_Conf, trial_ConfRT, trial_BlockCond, trialDDT,IA_1_DT, IA_2_DT]) \n",
    "\n",
    "#list(DDT_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stack both results (GSF and DDT) matrices together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDT_GSF_trial = np.column_stack((DDT_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDT_GSF_head = np.insert(DDT_matrix,  0, np.array(('Part', 'TrialN', 'PairID', 'LItem', 'LValue', 'RItem', 'RValue', 'ChosenITM', 'ChoiceRT', 'Conf', 'ConfRT', 'BlockCond', 'tDDT', 'lIA_DT', 'rIA_DT')), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "# 5. Display Summary Measures\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generate DataFrame for new summary table                                              \n",
    "DataFrameSum1=pd.DataFrame(DDT_GSF_head,columns=['Part', 'TrialN', 'PairID', 'LItem', 'LValue', 'RItem', 'RValue', 'ChosenITM', 'ChoiceRT', 'Conf', 'ConfRT', 'BlockCond', 'tDDT', 'lIA_DT', 'rIA_DT', 'tGSF','lastFixat'])\n",
    "DataFrameSum1=DataFrameSum1.drop(DataFrameSum1.index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_densities(DataFrameSum1,'Conf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice RT distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_densities(DataFrameSum1,'ChoiceRT',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial DDT distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_densities(DataFrameSum1,'tDDT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Item value distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right and left distributions are identical since we have the permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_densities(DataFrameSum1,'RValue') # Distribution of value of items "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial GSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_densities(DataFrameSum1,'tGSF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='zfam'></a>\n",
    "# 6. Calculate Z-scores \n",
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Z-values and other \n",
    "z_matrix=[]\n",
    "z_matrix_aux=[]\n",
    "\n",
    "for i in (DataFrameSum1['Part'].unique()):\n",
    "    Choicedata = DataFrameSum1.loc[DataFrameSum1['Part'] == str(i)]\n",
    "    # from v3 and on we define DV as  RIGHT - LEFT values of the items\n",
    "    pX_DV = - pd.to_numeric(Choicedata[\"LValue\"]) + pd.to_numeric(Choicedata[\"RValue\"])\n",
    "    pX_zDV = (pX_DV - np.mean(pX_DV))/np.std(pX_DV)\n",
    "    pX_absDV = abs(- pd.to_numeric(Choicedata['LValue']) + pd.to_numeric(Choicedata['RValue']))\n",
    "    pX_zAbsDV = (pX_absDV - np.mean(pX_absDV))/np.std(pX_absDV)\n",
    "    pX_zConf = (pd.to_numeric(Choicedata['Conf']) - np.mean(pd.to_numeric(Choicedata['Conf'])))/np.std(pd.to_numeric(Choicedata['Conf']))\n",
    "    pX_zChoiceRT = (pd.to_numeric(Choicedata['ChoiceRT'] )- np.mean(pd.to_numeric(Choicedata['ChoiceRT'])))/np.std(pd.to_numeric(Choicedata['ChoiceRT']))\n",
    "        \n",
    "    z_matrix_aux= np.column_stack((pX_DV,pX_zDV,pX_absDV,pX_zAbsDV,pX_zConf,pX_zChoiceRT))\n",
    "    \n",
    "    for  j in range(len(z_matrix_aux)):    \n",
    "        z_matrix.append(z_matrix_aux[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to aggregate table\n",
    "Final_trial = np.column_stack((DDT_GSF_trial,z_matrix))\n",
    "Final_head = np.insert(Final_trial,  0, np.array(('Part', 'TrialN', 'PairID', 'LItem', 'LValue', 'RItem', 'RValue', 'ChosenITM', 'ChoiceRT', 'Conf', 'ConfRT', 'BlockCond', 'tDDT', 'lIA_DT', 'rIA_DT', 'tGSF','lastFixat','DV','zDV','absDV','zAbsDV','zConf','zChoiceRT')), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save text file with DDT and GSF for each participant/trial\n",
    "#np.savetxt(\"DataFoodFramingNotebook_v1.csv\", Final_head, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract familiarity information from the BDM and add it to each trial in the choice step of the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load choice data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load choice trial values (saved in previous stage)\n",
    "\n",
    "#LOAD FROM FILE\n",
    "#data_exp1 = pd.read_csv('/Users/bdmlab/Google Drive/BdM Projects/FoodFraming_Amy/Analysis Scripts/DataFoodFramingNotebook_v1.csv') # The main data file for the first\n",
    "\n",
    "#CONTINUE USING DATASET FROM PREVIOUS STAGE\n",
    "data_exp1 = pd.DataFrame(Final_trial,columns = ['Part', 'TrialN', 'PairID', 'LItem', 'LValue', 'RItem', 'RValue', 'ChosenITM', 'ChoiceRT', 'Conf', 'ConfRT', 'BlockCond', 'tDDT', 'lIA_DT', 'rIA_DT', 'tGSF','lastFixat','DV','zDV','absDV','zAbsDV','zConf','zChoiceRT'])\n",
    "data_exp1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data in python - BDM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract BDM data from all the subject folders (Separated folder for each participant, each one containing the file \"BDM_RESULTS_FILE.txt\")\n",
    "\n",
    "#INPUT PATH FOR BDM_RESULTS_FILE folder\n",
    "main_path = \"/Users/bdmlab/OneDrive - University College London/PhD Projects/FoodFramingValue/Food Framing Data Raw/\"\n",
    "file_list = os.listdir(main_path)\n",
    "\n",
    "print(\"File List in Folder\")\n",
    "print(sorted(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_array = []\n",
    "\n",
    "for k in range(len(participant_names)):\n",
    "        BDM_filename = \"/BDM_RESULTS_FILE.txt\"\n",
    "        files =  main_path + participant_names[k] + BDM_filename\n",
    "        file_array.append(files) \n",
    "        \n",
    "print(\"number of files to load:\" ,len(file_array))        \n",
    "#file_array\n",
    "#Make sure that the paths are properly defined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define partipant names(numbers)\n",
    "participant = (data_exp1['Part'].unique())\n",
    "participant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding familiarity values and new variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FamLeft  = []\n",
    "FamRight = []\n",
    "Tot_Fam = []\n",
    "DFam = []\n",
    "FamChosen = []\n",
    "FamUnChosen = []\n",
    "ValChosen = []\n",
    "ValUnChosen = []\n",
    "Tot_Val = []\n",
    "Correct_val = []\n",
    "correct = []\n",
    "\n",
    "LValConfBDM = []\n",
    "LFamConfBDM = []\n",
    "RValConfBDM = []\n",
    "RFamConfBDM = []\n",
    "\n",
    "trial_count = 0\n",
    "\n",
    "#for cycle for participant\n",
    "for k in range(0,len(file_array)):\n",
    "    BDMframe = [pd.read_csv(file_array[k], sep=\"\\t\")]\n",
    "    BDMdata = pd.concat(BDMframe)\n",
    "    \n",
    "    #for cycle for trials \n",
    "    data_part = data_exp1.loc[(data_exp1['Part'] == str(participant[k]))]\n",
    "    trial_n = data_part['TrialN'].unique()\n",
    "    #print(data_part)\n",
    "\n",
    "    for  j in range(len(trial_n)):    \n",
    "        \n",
    "        #Pick identity left item  \n",
    "        L_item_name = data_part['LItem'][trial_count]\n",
    "        FamValue_L = BDMdata.loc[(BDMdata['BDM_ITM']== L_item_name),'BDM_FAM'].values[0]\n",
    "        Value_L = BDMdata.loc[(BDMdata['BDM_ITM']== L_item_name),'BDM_VAL'].values[0]\n",
    "        L_ValConfBDM = BDMdata.loc[(BDMdata['BDM_ITM']== L_item_name),'BDM_CONF'].values[0]\n",
    "        L_FamConfBDM = BDMdata.loc[(BDMdata['BDM_ITM']== L_item_name),'BDM_FAMCONF'].values[0]\n",
    "\n",
    "        #Pick identity right item\n",
    "        R_item_name = data_part['RItem'][trial_count]\n",
    "        FamValue_R = BDMdata.loc[(BDMdata['BDM_ITM']== R_item_name),'BDM_FAM'].values[0]\n",
    "        Value_R = BDMdata.loc[(BDMdata['BDM_ITM']== R_item_name),'BDM_VAL'].values[0]\n",
    "        R_ValConfBDM = BDMdata.loc[(BDMdata['BDM_ITM']== R_item_name),'BDM_CONF'].values[0]\n",
    "        R_FamConfBDM = BDMdata.loc[(BDMdata['BDM_ITM']== R_item_name),'BDM_FAMCONF'].values[0]\n",
    "\n",
    "        \n",
    "        BDMFamValue = BDMdata.loc[(BDMdata['BDM_ITM']== R_item_name),'BDM_FAM'].values[0]\n",
    "\n",
    "        ChosenITMtrial = data_part['ChosenITM'][trial_count]\n",
    "        BlockCondtrial = data_part['BlockCond'][trial_count]\n",
    "\n",
    "        if ChosenITMtrial == \"1\":  #Selected Item is Right\n",
    "            FamChosenT  = FamValue_R\n",
    "            FamUnChosenT  = FamValue_L\n",
    "            ValUnChosenT  = Value_L\n",
    "            ValChosenT  = Value_R\n",
    "            \n",
    "            if BlockCondtrial == \"1.0\": # Block is like, correct chosen is higher value\n",
    "                if Value_R >= Value_L:\n",
    "                    correct = 1\n",
    "                else: \n",
    "                    correct = 0\n",
    "            \n",
    "            if BlockCondtrial == \"2.0\" :# Block is dislike, correct chosen is lower value\n",
    "                if Value_L >= Value_R:\n",
    "                    correct = 1\n",
    "                else: \n",
    "                    correct = 0  \n",
    "                \n",
    "        else: # Selected item is Left\n",
    "            FamChosenT  = FamValue_L\n",
    "            FamUnChosenT  = FamValue_R\n",
    "            ValUnChosenT  = Value_R           \n",
    "            ValChosenT = Value_L\n",
    "            \n",
    "            if BlockCondtrial == \"1.0\": # Block is like, correct chosen is higher value\n",
    "                if Value_L >= Value_R:\n",
    "                    correct = 1\n",
    "                else: \n",
    "                    correct = 0\n",
    "            \n",
    "            if BlockCondtrial == \"2.0\" :# Block is dislike, correct chosen is lower value\n",
    "                if Value_R >= Value_L:\n",
    "                    correct = 1\n",
    "                else: \n",
    "                    correct = 0  \n",
    "        \n",
    "        FamValTot = FamValue_L + FamValue_R\n",
    "        DFamT =  FamValue_R - FamValue_L \n",
    "        \n",
    "        \n",
    "        ValTot = Value_L + Value_R\n",
    "        \n",
    "        #add to the rows with current trial measures\n",
    "        FamLeft.append(str(FamValue_L))\n",
    "        FamRight.append(str(FamValue_R))\n",
    "        Tot_Fam.append(str(FamValTot))\n",
    "        DFam.append(str(DFamT))\n",
    "        Tot_Val.append(str(ValTot))    \n",
    "        FamChosen.append(str(FamChosenT))\n",
    "        FamUnChosen.append(str(FamUnChosenT))\n",
    "        ValChosen.append(str(ValChosenT))\n",
    "        ValUnChosen.append(str(ValUnChosenT))\n",
    "        Correct_val.append(str(correct))\n",
    "        \n",
    "        LValConfBDM.append(str(L_ValConfBDM))\n",
    "        LFamConfBDM.append(str(L_FamConfBDM))\n",
    "        RValConfBDM.append(str(R_ValConfBDM))\n",
    "        RFamConfBDM.append(str(R_FamConfBDM))\n",
    "        \n",
    "        \n",
    "        \n",
    "        trial_count = trial_count + 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract familiarity information from the BDM and add it to each trial in the choice step of the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stack columns for the new familiarity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_aux= np.column_stack((DFam,FamLeft,FamRight,Tot_Fam,Tot_Val,FamChosen,FamUnChosen,ValChosen,ValUnChosen,Correct_val,LValConfBDM,LFamConfBDM,RValConfBDM,RFamConfBDM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check similar size in both tables\n",
    "print(len(table_aux))\n",
    "print(len(data_exp1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Fam_part = pd.DataFrame(table_aux,columns=['DFam','LFam','RFam','TotFam','TotVal','FamCh','FamUnCh','ValCh','ValUnCh','Correct','LValConfBDM','LFamConfBDM','RValConfBDM','RFamConfBDM'])\n",
    "new_data = pd.concat([data_exp1, Fam_part],axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Z-values for familiarity and (un)Chosen values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Agregate data matrix\n",
    "new_data = new_data.apply(pd.to_numeric, errors='ignore')\n",
    "z_matrix=[]\n",
    "z_matrix_aux=[]\n",
    "\n",
    "for i in unique(new_data[\"Part\"]):\n",
    "    Choicedata = new_data.loc[new_data['Part'] == i]\n",
    "    \n",
    "    # Z-score right and left item values \n",
    "    pX_LValue = pd.to_numeric(Choicedata['LValue'])\n",
    "    zLValue = (pX_LValue - np.mean(pX_LValue))/np.std(pX_LValue)\n",
    "    \n",
    "    pX_RValue = pd.to_numeric(Choicedata['RValue'])\n",
    "    zRValue = (pX_RValue - np.mean(pX_RValue))/np.std(pX_RValue)\n",
    "    \n",
    "    # Z-score for BDM confidence (value and familiarity)\n",
    "    pX_LValConfBDM = pd.to_numeric(Choicedata['LValConfBDM'])\n",
    "    zLValConfBDM = (pX_LValConfBDM - np.mean(pX_LValConfBDM))/np.std(pX_LValConfBDM)\n",
    "    \n",
    "    pX_LFamConfBDM = pd.to_numeric(Choicedata['LFamConfBDM'])\n",
    "    zLFamConfBDM = (pX_LFamConfBDM - np.mean(pX_LFamConfBDM))/np.std(pX_LFamConfBDM)\n",
    "    \n",
    "    pX_RValConfBDM = pd.to_numeric(Choicedata['RValConfBDM'])\n",
    "    zRValConfBDM = (pX_RValConfBDM - np.mean(pX_RValConfBDM))/np.std(pX_RValConfBDM)\n",
    "    \n",
    "    pX_RFamConfBDM = pd.to_numeric(Choicedata['RFamConfBDM'])\n",
    "    zRFamConfBDM = (pX_RFamConfBDM - np.mean(pX_RFamConfBDM))/np.std(pX_RFamConfBDM)\n",
    "    \n",
    "    # Z-score familiarity \n",
    "    pX_LFam = pd.to_numeric(Choicedata['LFam'])\n",
    "    zLFam = (pX_LFam - np.mean(pX_LFam))/np.std(pX_LFam)\n",
    "    \n",
    "    pX_RFam = pd.to_numeric(Choicedata['RFam'])\n",
    "    zRFam = (pX_RFam - np.mean(pX_RFam))/np.std(pX_RFam)\n",
    "    \n",
    "    pX_DFam = pd.to_numeric(Choicedata['DFam'])\n",
    "    zDFam = (pX_DFam - np.mean(pX_DFam))/np.std(pX_DFam)\n",
    "    \n",
    "    pX_totFam = pd.to_numeric(Choicedata['TotFam'])\n",
    "    z_totFam = (pX_totFam - np.mean(pX_totFam))/np.std(pX_totFam)\n",
    "\n",
    "    pX_totVal = pd.to_numeric(Choicedata['TotVal'])\n",
    "    z_totVal = (pX_totVal - np.mean(pX_totVal))/np.std(pX_totVal)\n",
    "    \n",
    "    pX_ValChosen = pd.to_numeric(Choicedata['ValCh'])\n",
    "    z_ValChosen = (pX_ValChosen - np.mean(pX_ValChosen))/np.std(pX_ValChosen)\n",
    "\n",
    "    pX_ValUnChosen = pd.to_numeric(Choicedata['ValUnCh'])\n",
    "    z_ValUnChosen = (pX_ValUnChosen - np.mean(pX_ValUnChosen))/np.std(pX_ValUnChosen)\n",
    "\n",
    "    pX_FamChosen = pd.to_numeric(Choicedata['FamCh'])\n",
    "    z_FamChosen = (pX_FamChosen - np.mean(pX_FamChosen))/np.std(pX_FamChosen)\n",
    "\n",
    "    pX_FamUnChosen = pd.to_numeric(Choicedata['FamUnCh'])\n",
    "    z_FamUnChosen = (pX_FamUnChosen - np.mean(pX_FamUnChosen))/np.std(pX_FamUnChosen)\n",
    "    \n",
    "    pX_tGSF = pd.to_numeric(Choicedata['tGSF'])\n",
    "    z_tGSF = (pX_tGSF - np.mean(pX_tGSF))/np.std(pX_tGSF)\n",
    "    \n",
    "    pX_tDDT = pd.to_numeric(Choicedata['tDDT'])\n",
    "    z_tDDT = (pX_tDDT - np.mean(pX_tDDT))/np.std(pX_tDDT)\n",
    "\n",
    "    absDDT = abs(z_tDDT)\n",
    "    abszDFam = abs(zDFam)\n",
    "\n",
    "\n",
    "    \n",
    "    z_matrix_aux= np.column_stack((zDFam,z_totFam,z_totVal,z_ValChosen,z_ValUnChosen,z_FamChosen,z_FamUnChosen, z_tGSF, z_tDDT, absDDT,abszDFam,zLValue,zRValue,zLFam,zRFam,zLValConfBDM,zLFamConfBDM,zRValConfBDM,zRFamConfBDM))\n",
    "    \n",
    "    for  j in range(len(z_matrix_aux)):    \n",
    "        z_matrix.append(z_matrix_aux[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zmatrix_part = pd.DataFrame(z_matrix,columns=['zDFam','zTotFam','zTotVal','zValCh','zValUnCh','zFamCh','zFamUnCh', 'zGSF', 'zDDT', 'absDDT','zAbsDFam','zLValue','zRValue','zLFam','zRFam','zLValConfBDM','zLFamConfBDM','zRValConfBDM','zRFamConfBDM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='split'></a>\n",
    "#  7. Including Median Split Data\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate median split for each participant and block conditions separatedly. \n",
    "#### We apply a split separating participants and block conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert the name of the variables to be used for the median split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Splits_headers = ['ConfSplit', 'GSFMedSplit','DDTMedSplit']\n",
    "Splits_variables = ['Conf', 'tGSF','tDDT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_matrix=[]\n",
    "median_matrix_aux=[]\n",
    "median_matrix_aux_cond=[]\n",
    "Split_cols = []\n",
    "\n",
    "for h in range(len(Splits_variables)):\n",
    "    median_matrix=[]\n",
    "    for i in unique(new_data[\"Part\"]):\n",
    "        median_matrix_aux=[]\n",
    "\n",
    "        for j in unique(new_data[\"BlockCond\"]):\n",
    "            #median_matrix_aux_cond=[]\n",
    "\n",
    "            #Extract data by participant and BlockCond (1: Like, 2: Dislike)\n",
    "            Splitdata = new_data.loc[new_data['Part'] == i]\n",
    "            Splitdata = Splitdata.loc[Splitdata['BlockCond'] == j]\n",
    "            \n",
    "            # Select the column to split\n",
    "            Splitdata = Splitdata.loc[:,['TrialN',Splits_variables[h]]]\n",
    "            SplitSort = Splitdata.values\n",
    "            # Sort for confidence column\n",
    "            SplitSort = SplitSort[np.argsort(SplitSort[:, 1])]\n",
    "            #print(SplitSort)\n",
    "            lenPart = len(Splitdata)\n",
    "            #print(lenPart)\n",
    "            # Generate column with median order\n",
    "            median_low = [0] * int (lenPart/2)\n",
    "            #print (median_low)\n",
    "            median_high = [1] * int (lenPart/2)\n",
    "            #print (median_high)\n",
    "            median_split_order = median_low + median_high\n",
    "            #Attach split median order column (1 and 0)\n",
    "            median_aux= np.column_stack((SplitSort,median_split_order))\n",
    "            #Sort back to trial number\n",
    "            SplitSorted = median_aux[np.argsort(median_aux[:, 0])]\n",
    "            #print(len(SplitSorted))\n",
    "            \n",
    "            for kk in range(len(SplitSorted)): \n",
    "                if len(median_matrix_aux) == 0 :\n",
    "                    median_matrix_aux = SplitSorted[0]\n",
    "                else:\n",
    "                    median_matrix_aux = np.vstack([median_matrix_aux, SplitSorted[kk]])\n",
    "        \n",
    "        #print(len(median_matrix_aux))\n",
    "        # sort after adding both conditions    \n",
    "        median_matrix_aux = median_matrix_aux[np.argsort(median_matrix_aux[:, 0])]\n",
    "        \n",
    "        #Extract only the column with the median split information\n",
    "        median_matrix_aux = median_matrix_aux[:,2]\n",
    "        \n",
    "        #append to median matrix for the participants\n",
    "        for k in range(len(median_matrix_aux)):    \n",
    "            median_matrix.append(median_matrix_aux [k])\n",
    "    \n",
    "    median_matrix = pd.DataFrame(median_matrix,columns=[Splits_headers[h]])\n",
    "    #print(len (median_matrix))\n",
    "    #add new column (new split variable columns) to the split matrix\n",
    "    if  h == 0:\n",
    "        Split_cols = median_matrix\n",
    "    else:\n",
    "        Split_cols = pd.concat([Split_cols,median_matrix ],axis =1)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join new familiarity and split datasets with the old trial information\n",
    "data_exp1 = pd.concat([new_data, Zmatrix_part, Split_cols],axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate variable names in Dataframe structure\n",
    "data_exp1.rename(index=str, columns={\"A\": \"a\", \"B\": \"c\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save text file with DDT and GSF for each participant/trial\n",
    "data_exp1.to_csv(\"Output/DataFoodFramingNotebook_BDMConfs_v2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='PartEx'></a>\n",
    "# 8. Participant Exclusion\n",
    "---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exclusion Criteria\n",
    "### As used in Folke et al. (2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Participants used less than 25% of the BDM scale.\n",
    "#### (2) Participants gave exactly the same BDM rating for more than 25% of the items.  [For our study adapted to 50%]\n",
    "#### (3) Participants used less than 25% of the confidence scale.\n",
    "#### (4) Participants gave exactly the same confidence rating for more than 25% of their choices. [For our study adapted to 50%]\n",
    "#### (*) adpted values to avoid eliminating too many participants from our study. Additionally, we consider that our slider moves in bigger jumps than the one used by Tomas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExclusionMatrix = []\n",
    "#for cycle for participant\n",
    "for k in range(0,len(file_array)):\n",
    " \n",
    "    # Checking for values \n",
    "    BDMframe = [pd.read_csv(file_array[k], sep=\"\\t\")]\n",
    "    BDMframe = pd.concat(BDMframe)\n",
    "    BDMvalue = BDMframe['BDM_VAL']\n",
    "    ValueRange = max(BDMvalue) - min(BDMvalue)\n",
    "    ValueCounts = BDMvalue.value_counts()\n",
    "    maxValueCounts = max(ValueCounts)\n",
    "    ValueCountsTotal = sum(int(i) for i in ValueCounts.values)\n",
    "    \n",
    "    PropnMostFreqVal = maxValueCounts / ValueCountsTotal\n",
    "    PropnRangeVal = ValueRange / 3\n",
    "    \n",
    "    # Checking for confidence\n",
    "    participant_data = data_exp1.loc[ (data_exp1['Part'] == k+1)]\n",
    "    ConfidencePart =   participant_data ['Conf']\n",
    "    \n",
    "    ConfRange = max(ConfidencePart) - min(ConfidencePart)\n",
    "    ConfCounts = ConfidencePart.value_counts()\n",
    "    \n",
    "    maxConfCounts = max(ConfCounts)\n",
    "    ConfCountsTotal = sum(int(i) for i in ConfCounts.values)\n",
    "    \n",
    "    PropnMostFreqConf = maxConfCounts / ConfCountsTotal\n",
    "    PropnRangeConf = ConfRange / 100\n",
    "\n",
    "    \n",
    "    ExclusionMatrix.append([k+1 , PropnMostFreqVal, PropnRangeVal, PropnMostFreqConf, PropnRangeConf]) \n",
    "\n",
    "#ExclusionMatrix=pd.DataFrame(ExclusionMatrix,columns=['Participant', 'ProportionMostFrequentBid', 'ProportionRangeBid','ProportionMostFrequentConfidence', 'ProportionRangeConfidence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ProportionMostFrequentBid : the proportion of all the BDM bids that were reported with the most frequent price (0: no repetition, 1: all items received the same bid price )\n",
    "##### ProportionRangeBid : proportion of the whole range (0 - 3) that was covered by the reported BDM bid price (0: no range, 1: whole range covered)\n",
    "##### ProportionMostFrequentConfidence : the proportion of all the confidence values (during the CHOICE STAGE) that has been reported with a single (most frequent) value (0: no repetition, 1: all items received the same confidence value )\n",
    "##### ProportionRangeConfidence : proportion of the whole range (0 - 100) that was covered by the confidence values (0: no range, 1: whole range covered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExclusionMatrix=pd.DataFrame(ExclusionMatrix,columns=['Participant', 'ProportionMostFrequentBid', 'ProportionRangeBid','ProportionMostFrequentConfidence', 'ProportionRangeConfidence'])\n",
    "ExclusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluded Participants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -Participant 6 excluded by criteria (1)\n",
    "#### -Participant 33 excluded by criteria (2)\n",
    "#### -NO participants excluded by criteria (3)\n",
    "#### -Participants 7, 17, 25, 30  excluded by criteria (4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortcut, load previous data (to avoid running from the top)\n",
    "# data_exp1 = pd.read_csv('/Users/pradyumna/Documents/gitDocs/FF2018/Output/DataFoodFramingNotebook_v1.csv') \n",
    "# data_exp1 = pd.read_csv('/Users/bdmlab/Documents/GiTs/FF2018/Output/DataFoodFramingNotebook_v1.csv') \n",
    "\n",
    "for i in [6, 7, 17, 25, 30, 33]:\n",
    "    data_exp1 = data_exp1[(data_exp1.Part != i)]\n",
    "data_exp1[\"Part\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_exp1[\"Part\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exp1.to_csv(\"Output/DataFoodFramingNotebook_ExcludedPart_v1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Additionally, it was checked that trials in which participants did not fixate in any of the items were were skipped (trials fulfiling this criteria were found only in participant 'p40.edf')*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "_____________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ImpPkg'></a>\n",
    "# 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pradyumna/anaconda3/lib/python3.7/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['copy', 'pylab']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.frame import DataFrame as DF\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "#np.random.seed(sum(map(ord, \"distributions\")))\n",
    "from sklearn import linear_model  # packages for the logistic regression function to plot the logistic regression \n",
    "from sklearn.linear_model import LogisticRegression # packages for the logistic regression function to plot the logistic regression \n",
    "import scipy\n",
    "from scipy import stats, integrate\n",
    "from scipy.stats import mode\n",
    "from scipy.stats.stats import pearsonr # Pearson's correlation\n",
    "from copy import copy as copy\n",
    "import operator as operator\n",
    "import pylab\n",
    "\n",
    "# Plotting tools\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "%pylab inline\n",
    "figsize(5, 5)\n",
    "\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2\n",
    "\n",
    "\n",
    "%load_ext rpy2.ipython\n",
    "# Set up interface with R\n",
    "# Make it easy to set and find values in a multi-index DF\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# Set up interface with R\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import warnings\n",
    "from rpy2.rinterface import RRuntimeWarning\n",
    "warnings.filterwarnings(\"ignore\", category=RRuntimeWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NULL\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "# Use in case the libraries are not available in the system\n",
    "#install.packages(\"lme4\")\n",
    "#install.packages(\"car\")\n",
    "#install.packages(\"ggplot2\")\n",
    "#install.packages(\"broom\")\n",
    "#install.packages(\"arm\")\n",
    "#install.packages(\"ggplot2\")\n",
    "#install.packages(\"optimx\")\n",
    "#install.packages(\"multcomp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error in library(ggplot2) : there is no package called ggplot2\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "library(lme4)\n",
    "library(car)\n",
    "library(optimx)\n",
    "library(ggplot2)\n",
    "library(MASS)\n",
    "library(pbkrtest)\n",
    "library(broom)\n",
    "library(dplyr)\n",
    "library(dplyr)\n",
    "library(reshape2)\n",
    "library(arm)\n",
    "library(multcomp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LoadFunc'></a>\n",
    "# 2. Defining Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taken from Folke et al. (2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def participant_densities(data, var, xlim=(0,100), rug=True):\n",
    "    # a counter that tells us where a given participant's data should be plotted\n",
    "    order = 1\n",
    "\n",
    "    # a list of all the participants in the dataset\n",
    "    participants = data.loc[:, 'Part'].unique()\n",
    "\n",
    "    # defining the figure size\n",
    "    sns.set_style('white')\n",
    "    fig = figure(figsize=(15,70))\n",
    "\n",
    "    for x in participants:\n",
    "        # defining the sub figures\n",
    "            sub={}\n",
    "            sub['%s' % x] = plt.subplot(len(participants)/2, 3, order)\n",
    "            sns.kdeplot(data.loc[data['Part'] == x, var].values, ax = sub['%s' % x], shade=True)\n",
    "            #if rug==True:\n",
    "            #    sns.rugplot(data.loc[data['Part'] == x, var].values, ax = sub['%s' % x])\n",
    "            sub['%s' % x].set_title('participant %s' % x)\n",
    "            #sub['%s' % x].set_xlim(xlim)\n",
    "            order += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split variable into participantwise quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsplit(DF, input, quantiles):\n",
    "    qvalues = pd.qcut(DF[input], quantiles, labels = range(1, quantiles+1))\n",
    "    return qvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full simple logistic graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticplot_all (moderator, modhigh, modlow, data, xaxis='zDV', yaxis='G_choice', ylab='P(Chose Reference Item)', xlab='DV (Z-score)',\n",
    "                 modhighcol='#000000', modlowcol='#AAAAAA', title='empty', parvar='SubNo'):\n",
    "    \n",
    "    sns.set(font_scale=1.5, style='white')\n",
    "    fig = figure(figsize=(8,7))\n",
    "    fig.set_facecolor('white')\n",
    "    \n",
    "    # defining the sigmoid function\n",
    "    def model(x):\n",
    "        y = 1 / (1 + np.exp(-x))\n",
    "        return y\n",
    "    \n",
    "    sub = plt.subplot()\n",
    "\n",
    "\n",
    "    #run the classifier\n",
    "    clf = linear_model.LogisticRegression(C=1e5)\n",
    "\n",
    "    # Paula used these dictionaries to store the values of the predictive lines for all the participants.\n",
    "    logit_low = {}\n",
    "    logit_high = {}\n",
    "\n",
    "    # I think this defines the problem space\n",
    "    X_test = np.linspace(-5,10,300)\n",
    "\n",
    "    # fitting the predictive logistic model for the low_confidence trials, for a participant specified by x\n",
    "    # first I specify the value difference right - left, then I specify the choices, left or right\n",
    "    clf.fit(data.loc[data[(data[moderator]==0)].index, xaxis][:, np.newaxis],\n",
    "            data.loc[data[(data[moderator]==0)].index, yaxis])\n",
    "    logit_low = model(X_test*clf.coef_ + clf.intercept_).ravel()\n",
    "\n",
    "\n",
    "    # fitting the predictive logistic model for the high_confidence trials, for a participant specified by x\n",
    "    # first I specify the value difference right - left, then I specify the choices, left or right\n",
    "    clf.fit(data.loc[data[(data[moderator]==1)].index, xaxis][:, np.newaxis],\n",
    "            data.loc[data[(data[moderator]==1)].index, yaxis])\n",
    "    logit_high = model(X_test * clf.coef_ + clf.intercept_).ravel()\n",
    "\n",
    "    #Plotting the predictive lines\n",
    "    line_high = sub.plot(X_test, logit_high, color=modhighcol, linewidth=5, label=modhigh, zorder=6)\n",
    "    line_low = sub.plot(X_test, logit_low, color=modlowcol, linewidth=5, label=modlow, zorder=5)\n",
    "\n",
    "\n",
    "    #Plotting the binned data\n",
    "    data['DVBin2'] = data.groupby(parvar).apply(parsplit, input=xaxis, quantiles=4).values\n",
    "    \n",
    "    # determine the x coordinates\n",
    "    x_cords= data.groupby('DVBin2')[xaxis].mean()\n",
    "    \n",
    "    # determine low y coordinates\n",
    "    y_cords_low = data.loc[(data[moderator]==0), :].groupby('DVBin2')[yaxis].mean().values\n",
    "    \n",
    "    # determine low y standard errors\n",
    "    test = pd.DataFrame(data.loc[(data[moderator]==0), :].groupby(['DVBin2', parvar])[yaxis].mean()).reset_index()\n",
    "    y_low_error = test.groupby('DVBin2')[yaxis].std()/np.sqrt(len(test[parvar].unique()))\n",
    "    \n",
    "    \n",
    "    # determine high y coordinates\n",
    "    y_cords_high = data.loc[(data[moderator]==1), :].groupby('DVBin2')[yaxis].mean().values\n",
    "    \n",
    "    # determine high y standard errors\n",
    "    test2 = pd.DataFrame(data.loc[data[moderator]==1, :].groupby(['DVBin2', parvar])[yaxis].mean()).reset_index()\n",
    "    y_high_error = test2.groupby('DVBin2')[yaxis].std()/np.sqrt(len(test[parvar].unique()))\n",
    "    \n",
    "    # plot the low points\n",
    "    plt.scatter(x_cords, y_cords_low, c=modlowcol, marker='D', s=60, zorder=1)\n",
    "    # plot low error bars\n",
    "    plt.errorbar(x_cords, y_cords_low, yerr=y_low_error, fmt='o', zorder=3, c=modlowcol)\n",
    "    \n",
    "    # plot the high points\n",
    "    plt.scatter(x_cords, y_cords_high, c=modhighcol, marker='o', s=60, zorder=2)\n",
    "    # plot high error bars\n",
    "    plt.errorbar(x_cords, y_cords_high, yerr=y_high_error, fmt='o', zorder=4, c=modhighcol)\n",
    "    \n",
    "    \n",
    "    # Set Labels\n",
    "    sub.set_ylabel(ylab, fontsize=30)\n",
    "    sub.set_xlabel(xlab, fontsize=30)\n",
    "\n",
    "    # Set Ticks\n",
    "    sub.set_xticks((-5,-3,-1,1,3,5))\n",
    "    sub.set_yticks((0,0.25,0.5,0.75,1))\n",
    "    sub.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "    # Set Limits\n",
    "    sub.set_ylim(-0.01, 1.01)\n",
    "    sub.set_xlim(-5, 5)\n",
    "\n",
    "    # Set Title\n",
    "    if title == 'empty':\n",
    "        sub.set_title('')\n",
    "    else:\n",
    "        sub.set_title(title)\n",
    "    \n",
    "    sub.legend(loc=2, prop={'size':20})\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full simple logistic graph (no bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticplot_simpl (moderator, modhigh, modlow, data, xaxis='zDV', yaxis='G_choice', ylab='P(Chose Reference Item)', xlab='DV (Z-score)',\n",
    "                 modhighcol='#000000', modlowcol='#AAAAAA', title='empty', parvar='SubNo'):\n",
    "    \n",
    "    sns.set(font_scale=1.5, style='white')\n",
    "    fig = figure(figsize=(8,7))\n",
    "    fig.set_facecolor('white')\n",
    "    \n",
    "    # defining the sigmoid function\n",
    "    def model(x):\n",
    "        y = 1 / (1 + np.exp(-x))\n",
    "        return y\n",
    "    \n",
    "    sub = plt.subplot()\n",
    "\n",
    "\n",
    "    #run the classifier\n",
    "    clf = linear_model.LogisticRegression(C=1e5)\n",
    "\n",
    "    # Paula used these dictionaries to store the values of the predictive lines for all the participants.\n",
    "    logit_low = {}\n",
    "    logit_high = {}\n",
    "\n",
    "    # I think this defines the problem space\n",
    "    X_test = np.linspace(-5,10,300)\n",
    "\n",
    "    # fitting the predictive logistic model for the low_confidence trials, for a participant specified by x\n",
    "    # first I specify the value difference right - left, then I specify the choices, left or right\n",
    "    clf.fit(data.loc[data[(data[moderator]==0)].index, xaxis][:, np.newaxis],\n",
    "            data.loc[data[(data[moderator]==0)].index, yaxis])\n",
    "    logit_low = model(X_test*clf.coef_ + clf.intercept_).ravel()\n",
    "    print ('Low measure coef',clf.coef_)\n",
    "    \n",
    "    # fitting the predictive logistic model for the high_confidence trials, for a participant specified by x\n",
    "    # first I specify the value difference right - left, then I specify the choices, left or right\n",
    "    clf.fit(data.loc[data[(data[moderator]==1)].index, xaxis][:, np.newaxis],\n",
    "            data.loc[data[(data[moderator]==1)].index, yaxis])\n",
    "    logit_high = model(X_test * clf.coef_ + clf.intercept_).ravel()\n",
    "    print ('High measure coef',clf.coef_)\n",
    "\n",
    "\n",
    "\n",
    "    #Plotting the predictive lines\n",
    "    line_high = sub.plot(X_test, logit_high, color=modhighcol, linewidth=5, label=modhigh, zorder=6)\n",
    "    line_low = sub.plot(X_test, logit_low, color=modlowcol, linewidth=5, label=modlow, zorder=5) \n",
    "    \n",
    "    # Set Labels\n",
    "    sub.set_ylabel(ylab, fontsize=30)\n",
    "    sub.set_xlabel(xlab, fontsize=30)\n",
    "\n",
    "    # Set Ticks\n",
    "    sub.set_xticks((-5,-3,-1,1,3,5))\n",
    "    sub.set_yticks((0,0.25,0.5,0.75,1))\n",
    "    sub.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "    # Set Limits\n",
    "    sub.set_ylim(-0.01, 1.01)\n",
    "    sub.set_xlim(-5, 5)\n",
    "\n",
    "    # Set Title\n",
    "    if title == 'empty':\n",
    "        sub.set_title('')\n",
    "    else:\n",
    "        sub.set_title(title)\n",
    "    \n",
    "    sub.legend(loc=2, prop={'size':20})\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Coefficients Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Coefpresplot(regtable, intercept=False, title='Regression Coefficients', size='big', ylimits=(), ymultiple=0.5, ticklabsize=25, n_ET_predictors = 0):\n",
    "\n",
    "    # Import itertools so that we can iterate through the colours\n",
    "    import itertools\n",
    "    \n",
    "    # Import locators so that we can tidy up the yaxis\n",
    "    from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "    \n",
    "    # rounding function to get edges to even 0.5 values\n",
    "    def round_to(n, precision):\n",
    "        correction = 0.5 if n >= 0 else -0.5\n",
    "        return int( n/precision+correction ) * precision\n",
    "\n",
    "    def round_to_5(n):\n",
    "        return round_to(n, 0.5)\n",
    "\n",
    "    # Set seaborn style for the plot\n",
    "    sns.set(style='white')\n",
    "    \n",
    "    # Generate the figure\n",
    "    if size=='big':\n",
    "        fig = plt.figure(figsize=[20,8])\n",
    "    elif size=='long':\n",
    "        fig = plt.figure(figsize=[20,4])\n",
    "    elif size=='narrow':\n",
    "        fig = plt.figure(figsize=[10,4])\n",
    "        \n",
    "    fig.suptitle(title, fontsize=20)\n",
    "    gs = GridSpec(1,1,bottom=0.18,left=0.18,right=0.82)\n",
    "    ax = fig.add_subplot(gs[0,0])\n",
    "    \n",
    "    # Set axis limits based on whether to include the intercept or not    \n",
    "    if intercept == True:\n",
    "        XLim = (0.75, len(regtable.columns) + 0.25)\n",
    "        YLim = (round_to_5(regtable.loc['CImin', :].min()-0.1), round_to_5(regtable.loc['CImax', :].max()+0.2))\n",
    "    else:\n",
    "        XLim = (0.75, len(regtable.columns) - 0.75)\n",
    "        YLim = (round_to_5(regtable.loc['CImin', regtable.columns[1]:].min()-0.2), round_to_5(regtable.loc['CImax', regtable.columns[1]:].max()+0.2))\n",
    "    if ylimits != ():\n",
    "        YLim = ylimits\n",
    "    ax.set_xlim(XLim)\n",
    "    ax.set_ylim(YLim)\n",
    "    \n",
    "    # Draw a line through the 0-value on the y-axis\n",
    "    line = ax.plot(XLim, [0, 0], color='black', ls='--', alpha = 0.5, lw=3)\n",
    "    \n",
    "    \n",
    "    # If intercept is true, plot the coefficient for the intercept\n",
    "    if intercept == True:\n",
    "        Coefficients = regtable.columns\n",
    "    else:\n",
    "        Coefficients = regtable.columns[1:]\n",
    "        \n",
    "    # Determine the colours for the coefficients based on the n_ET_variable\n",
    "    n_predictors = len(Coefficients)\n",
    "    n_non_ET_predictors = n_predictors - n_ET_predictors\n",
    "    \n",
    "    colourlist = ['#000000'] * n_non_ET_predictors + ['#03719c'] * n_ET_predictors\n",
    "    \n",
    "        \n",
    "    # Plot all the coefficients with 95% CI\n",
    "    position = 0\n",
    "    for Coefficient in Coefficients:\n",
    "        position += 1\n",
    "        ax.plot(position, regtable.loc['coefficient', Coefficient], marker='o', ms=8, color=colourlist[position-1],)\n",
    "        ax.errorbar(position, regtable.loc['coefficient', Coefficient],\n",
    "                    yerr=regtable.loc['se', Coefficient]*1.96, lw=2, color=colourlist[position-1])\n",
    "\n",
    "    # Setting the x-axis major tick's location\n",
    "    ax.set_xticks(range(1, position+1))\n",
    "    \n",
    "    # set the y-axis major tick position\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(ymultiple))\n",
    "\n",
    "\n",
    "    # Setting the x-axis major tick's label\n",
    "    if intercept == True:\n",
    "        ax.set_xticklabels(regtable.columns, rotation=40)\n",
    "    else: \n",
    "        ax.set_xticklabels(regtable.columns[1:], rotation=40)\n",
    "    \n",
    "    ax.tick_params(axis='both', which='major', labelsize=ticklabsize)\n",
    "    ax.set_ylabel('Fixed Effects Coefficients', fontsize=18)\n",
    "    \n",
    "    # Autoformats the ticklabels for the xaxis\n",
    "    fig.autofmt_xdate()\n",
    "    \n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Coefpresplot2(regtable,regtable2, intercept=False, title='Regression Coefficients', size='big', ylimits=(), ymultiple=0.5, ticklabsize=25, n_ET_predictors = 0):\n",
    "\n",
    "    # Import itertools so that we can iterate through the colours\n",
    "    import itertools\n",
    "    \n",
    "    # Import locators so that we can tidy up the yaxis\n",
    "    from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "    \n",
    "    # rounding function to get edges to even 0.5 values\n",
    "    def round_to(n, precision):\n",
    "        correction = 0.5 if n >= 0 else -0.5\n",
    "        return int( n/precision+correction ) * precision\n",
    "\n",
    "    def round_to_5(n):\n",
    "        return round_to(n, 0.5)\n",
    "\n",
    "    # Set seaborn style for the plot\n",
    "    sns.set(style='white')\n",
    "    \n",
    "    # Generate the figure\n",
    "    if size=='big':\n",
    "        fig = plt.figure(figsize=[20,8])\n",
    "    elif size=='long':\n",
    "        fig = plt.figure(figsize=[20,4])\n",
    "    elif size=='narrow':\n",
    "        fig = plt.figure(figsize=[10,4])\n",
    "        \n",
    "    fig.suptitle(title, fontsize=20)\n",
    "    gs = GridSpec(1,1,bottom=0.18,left=0.18,right=0.82)\n",
    "    ax = fig.add_subplot(gs[0,0])\n",
    "    \n",
    "\n",
    "    # Set axis limits based on whether to include the intercept or not    \n",
    "    \n",
    "       \n",
    "    \n",
    "    if intercept == True:\n",
    "        \n",
    "        XLim = (0.75, len(regtable.columns) + 0.25)\n",
    "        \n",
    "        #Determine the Ymax and Ymin for both regresions results\n",
    "        if round_to_5(regtable.loc['CImin', :].min()) < round_to_5(regtable2.loc['CImin', :].min()):\n",
    "            Ymin = round_to_5(regtable.loc['CImin', :].min()-0.5)\n",
    "        else :\n",
    "            Ymin = round_to_5(regtable2.loc['CImin',:].min()-0.5)\n",
    "        if round_to_5(regtable.loc['CImax', :].max()) > round_to_5(regtable2.loc['CImax', :].max()):\n",
    "            Ymax = round_to_5(regtable.loc['CImax', :].max()+0.5)\n",
    "        else :\n",
    "            Ymax = round_to_5(regtable2.loc['CImax', :].max()+0.5)      \n",
    "\n",
    "        YLim = (Ymin, Ymax)    \n",
    "    else:\n",
    "        XLim = (0.75, len(regtable.columns) - 0.75)\n",
    "        \n",
    "        #Determine the Ymax and Ymin for both regresions results\n",
    "        if round_to_5(regtable.loc['CImin', regtable.columns[1]:].min()) < round_to_5(regtable2.loc['CImin', regtable.columns[1]:].min()):\n",
    "            Ymin = round_to_5(regtable.loc['CImin', regtable.columns[1]:].min()-0.5)\n",
    "        else :\n",
    "            Ymin = round_to_5(regtable2.loc['CImin',regtable.columns[1]:].min()-0.5)\n",
    "        if round_to_5(regtable.loc['CImax', regtable.columns[1]:].max()) > round_to_5(regtable2.loc['CImax', regtable.columns[1]:].max()):\n",
    "            Ymax = round_to_5(regtable.loc['CImax', regtable.columns[1]:].max()+0.5)\n",
    "        else :\n",
    "            Ymax = round_to_5(regtable2.loc['CImax', regtable.columns[1]:].max()+0.5)      \n",
    "        \n",
    "        YLim = (Ymin, Ymax)\n",
    "    \n",
    "    if ylimits != ():\n",
    "        YLim = ylimits\n",
    "    ax.set_xlim(XLim)\n",
    "    ax.set_ylim(YLim)\n",
    "    \n",
    "    # Draw a line through the 0-value on the y-axis\n",
    "    line = ax.plot(XLim, [0, 0], color='black', ls='--', alpha = 0.5, lw=3)\n",
    "    \n",
    "    \n",
    "    # If intercept is true, plot the coefficient for the intercept\n",
    "    # both regtable should have the same regressors (and in the same order)\n",
    "\n",
    "    if intercept == True:\n",
    "        Coefficients = regtable.columns\n",
    " #       Coefficients2 = regtable2.columns\n",
    "    else:\n",
    "        Coefficients = regtable.columns[1:]\n",
    "#        Coefficients2 = regtable2.columns[1:]\n",
    "        \n",
    "    # Determine the colours for the coefficients based on the n_ET_variable\n",
    "    n_predictors = len(Coefficients)\n",
    "    n_non_ET_predictors = n_predictors - n_ET_predictors\n",
    "    \n",
    "    colourlist = ['#000000'] * n_non_ET_predictors + ['#03719c'] * n_ET_predictors\n",
    "    \n",
    "        \n",
    "    # Plot all the coefficients with 95% CI\n",
    "    position = 0\n",
    "    for Coefficient in Coefficients:\n",
    "        position += 1\n",
    "        ax.plot(position-0.1, regtable.loc['coefficient', Coefficient], marker='o', ms=8, color='blue',label = 'Like')\n",
    "        ax.plot(position+0.1, regtable2.loc['coefficient', Coefficient], marker='X', ms=8, color='red', label = 'Dislike')\n",
    "\n",
    "\n",
    "        ax.errorbar(position-0.1, regtable.loc['coefficient', Coefficient],\n",
    "                    yerr=regtable.loc['se', Coefficient]*1.96, lw=2, color='blue')\n",
    "        ax.errorbar(position+0.1, regtable2.loc['coefficient', Coefficient],\n",
    "                    yerr=regtable2.loc['se', Coefficient]*1.96, lw=2, color='red')\n",
    "        \n",
    "        if position == 1:\n",
    "            ax.legend( prop={'size': 20})\n",
    "\n",
    " \n",
    "    # Setting the x-axis major tick's location\n",
    "    ax.set_xticks(range(1, position+1))\n",
    "    \n",
    "    # set the y-axis major tick position\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(ymultiple))\n",
    "\n",
    "    # Setting the x-axis major tick's label\n",
    "    if intercept == True:\n",
    "        ax.set_xticklabels(regtable.columns, rotation=40)\n",
    "    else: \n",
    "        ax.set_xticklabels(regtable.columns[1:], rotation=40)\n",
    "    \n",
    "    ax.tick_params(axis='both', which='major', labelsize=ticklabsize)\n",
    "    ax.set_ylabel('Fixed Effects Coefficients', fontsize=18)\n",
    "    \n",
    "    # Autoformats the ticklabels for the xaxis\n",
    "    fig.autofmt_xdate()\n",
    "\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regtable(fix, fix_se, names):\n",
    "    fixed_betas = DF(np.array(fix)); fixed_betas = fixed_betas.transpose(); fixed_betas.columns = names\n",
    "    fixed_betas.loc[1] = np.array(fix_se)\n",
    "    fixed_betas.loc[2] = fixed_betas.loc[0] - (fixed_betas.loc[1]*1.96)\n",
    "    fixed_betas.loc[3] = fixed_betas.loc[0] + (fixed_betas.loc[1]*1.96)\n",
    "    fixed_betas.index = ['coefficient', 'se', 'CImin', 'CImax']\n",
    "    return fixed_betas"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#999; background:#fff;\">\n",
    "Created with Jupyter, delivered by Fastly, rendered by Rackspace.\n",
    "</footer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
